{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiarayosianti/Spatial-SCINet/blob/main/Model_Spatial_SCINet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import modules"
      ],
      "metadata": {
        "id": "wa7cakQbBb0-"
      },
      "id": "wa7cakQbBb0-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import modules"
      ],
      "metadata": {
        "id": "TgigNIxO2iRp"
      },
      "id": "TgigNIxO2iRp"
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from math import sqrt\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from module.train_model import train_conv2d_scinet\n",
        "from module.SpatialSCINet import Conv2D_SCINet"
      ],
      "metadata": {
        "id": "0WbvXkLgBfEp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "0WbvXkLgBfEp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess data"
      ],
      "metadata": {
        "id": "1fIMzkRBAvB4"
      },
      "id": "1fIMzkRBAvB4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform data scaling"
      ],
      "metadata": {
        "id": "WmPeYyN22f3F"
      },
      "id": "WmPeYyN22f3F"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/new_df_clean_pm10_meteorologi.csv')\n",
        "data_x = dataset[['Tavg',\t'RH_avg',\t'ff_avg',\t'RR']]\n",
        "data_y_pm10 = dataset[['PM10']]\n",
        "\n",
        "scaler_x = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "data_x_scaled = scaler_x.fit_transform(data_x)\n",
        "data_y_scaled_pm10 = scaler_y.fit_transform(data_y_pm10)\n",
        "\n",
        "data_x_scaled = pd.DataFrame(data_x_scaled, columns = ['Tavg',\t'RH_avg',\t'ff_avg',\t'RR'])\n",
        "data_y_scaled = pd.DataFrame(data_y_scaled_pm10, columns = ['PM10'])\n",
        "\n",
        "data_scaled = dataset[['Tanggal','kota', 'Latitude', 'Longitude']]\n",
        "data_scaled = pd.concat([data_scaled, data_y_scaled, data_x_scaled], axis=1)\n",
        "\n",
        "print(data_scaled.shape)\n",
        "data_scaled.head()"
      ],
      "metadata": {
        "id": "9x-r9zuuabFh"
      },
      "id": "9x-r9zuuabFh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_x_scaled.max(), data_y_scaled.max(), data_x_scaled.min(), data_y_scaled.min()"
      ],
      "metadata": {
        "id": "LI1SDluM83Zw"
      },
      "id": "LI1SDluM83Zw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.max(), dataset.min()"
      ],
      "metadata": {
        "id": "HU-K9NBeKXRo"
      },
      "id": "HU-K9NBeKXRo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function\n"
      ],
      "metadata": {
        "id": "AOXgKHsuyf8B"
      },
      "id": "AOXgKHsuyf8B"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating 4-dimensional array\n",
        "def create_dataset(Latitude, Longitude, WINDOW_SIZE, horizon):\n",
        "    location_current = data_scaled.loc[((data_scaled['Latitude'] == Latitude) & (data_scaled['Longitude'] == Longitude))]\n",
        "    variables = location_current[['PM10',\t'Tavg',\t'RH_avg',\t'ff_avg',\t'RR']]\n",
        "    variables = np.array(variables)\n",
        "    X, Y = [], []\n",
        "    for i in range(len(variables) - WINDOW_SIZE - horizon):\n",
        "        X.append(variables[i:(i + WINDOW_SIZE), 1:][::-1])\n",
        "        Y.append(variables[(i + WINDOW_SIZE):(i + WINDOW_SIZE + horizon), 0]) # PM10 is in column index 0\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "# Split the dataset for train, validation, and test data\n",
        "def split_data(X_array, Y_array, train_frac, val_frac, test_frac, N_samples):\n",
        "    train_X, val_X, test_X = X_array[:int(train_frac * N_samples), :], X_array[int(train_frac * N_samples):int((val_frac + train_frac) * N_samples), :], X_array[int((val_frac + train_frac) * N_samples):, :]\n",
        "    train_y, val_y, test_y = Y_array[:int(train_frac * N_samples), :], Y_array[int(train_frac * N_samples):int((val_frac + train_frac) * N_samples), :], Y_array[int((val_frac + train_frac) * N_samples):, :]\n",
        "    return train_X, val_X, test_X, train_y, val_y, test_y\n",
        "\n",
        "\n",
        "# Visualiza the train loss vs validation loss\n",
        "def learning_curve(history_model, model_name):\n",
        "  plt.plot(history_model.history['loss'], label = 'train')\n",
        "  plt.plot(history_model.history['val_loss'], label = 'val')\n",
        "  plt.title('Train and Validation Loss: {}'.format(model_name))\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Inverse transform the ground truth y and predicted value\n",
        "def invers_trans(model_name, pred_y, true_y):\n",
        "  inv_pred_y = []\n",
        "  for yhat in pred_y:\n",
        "    inv_yhat = yhat.reshape((yhat.shape[0]*yhat.shape[1], yhat.shape[2]))\n",
        "    inv_yhat = scaler_y.inverse_transform(inv_yhat)\n",
        "    inv_pred_y.append(inv_yhat)\n",
        "  inv_true_y = true_y.reshape((true_y.shape[0]*true_y.shape[1], true_y.shape[2]))\n",
        "  inv_true_y = scaler_y.inverse_transform(inv_true_y)\n",
        "  return inv_pred_y, inv_true_y\n",
        "\n",
        "\n",
        "# Evaluation metrics\n",
        "def evaluation_metrics(y_actual, y_predict, model_name):\n",
        "  em_summary = pd.DataFrame(columns = ['model_name', 'MAPE', 'MAE', 'RMSE'])\n",
        "  for idx, y_predict_seq in enumerate(y_predict):\n",
        "    new_row = {'model_name': model_name[idx],\n",
        "            'MAPE': mean_absolute_percentage_error(y_actual, y_predict_seq)*100,\n",
        "            'MAE': mean_absolute_error(y_actual, y_predict_seq),\n",
        "            'RMSE': sqrt(mean_squared_error(y_actual, y_predict_seq))}\n",
        "    em_summary.loc[idx] = new_row\n",
        "  return em_summary\n",
        "\n",
        "\n",
        "# Search the best model\n",
        "def the_best_model(df):\n",
        "    df['metrics'] = df[['RMSE', 'MAE', 'MAPE']].sum(axis=1)\n",
        "    best_model = df[df['metrics'] == df['metrics'].min()]['model_name']\n",
        "    return best_model.values.tolist()\n",
        "\n",
        "\n",
        "# Check the level number that compatible with the window size\n",
        "def check_input(comb_params):\n",
        "  rule_1 = []\n",
        "  rule_2 = []\n",
        "  rule_all = []\n",
        "  for i in range(len(comb_params)):\n",
        "        # rule num 1\n",
        "        if comb_params['input_len'][i] % 2**comb_params['num_levels'][i] == 0:\n",
        "          rule_1.append('ok')\n",
        "        else:\n",
        "          rule_1.append('bad')\n",
        "        # rule num 2\n",
        "        if (comb_params['input_len'][i] / 2**comb_params['num_levels'][i]) % 2 == 0:\n",
        "          rule_2.append('ok')\n",
        "        else:\n",
        "          rule_2.append('bad')\n",
        "  df_rule = pd.DataFrame()\n",
        "  df_rule['rule_1'] = rule_1\n",
        "  df_rule['rule_2'] = rule_2\n",
        "  for i in range(len(df_rule)):\n",
        "        # all rules\n",
        "        if df_rule['rule_1'][i] == 'ok' and df_rule['rule_2'][i] == 'ok':\n",
        "          rule_all.append('use it!')\n",
        "        else:\n",
        "          rule_all.append('bad')\n",
        "  df_rule['rule_all'] = rule_all\n",
        "  df_rule2 = pd.concat([comb_params, df_rule], axis = 1)\n",
        "  df_rule2 = df_rule2.sort_values(by=['rule_all', 'input_len'], ascending=[False,True])\n",
        "  return df_rule2\n",
        "\n",
        "the_param = {'input_len' :  [24, 30, 48], 'num_levels' : [2,\t3,\t4,\t5]}\n",
        "the_param_combinations = list(ParameterGrid(the_param))\n",
        "df_param = pd.DataFrame(the_param_combinations)\n",
        "check_result = check_input(df_param)\n",
        "check_result"
      ],
      "metadata": {
        "id": "mgyWzdOgH02K"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mgyWzdOgH02K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelling"
      ],
      "metadata": {
        "id": "jOy-RYakTSro"
      },
      "id": "jOy-RYakTSro"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating 4D data shape\n",
        "latitude = data_scaled['Latitude'].unique()\n",
        "longitude = data_scaled['Longitude'].unique()\n",
        "coords = list(zip(latitude, longitude))\n",
        "print(coords)\n",
        "\n",
        "df_X, df_Y = [], []\n",
        "for lat, lon in tqdm(coords):\n",
        "    a, b = create_dataset(Latitude = lat,\tLongitude = lon, WINDOW_SIZE = 24, horizon = 24)\n",
        "    df_X.append(a)\n",
        "    df_Y.append(b)\n",
        "\n",
        "df_X = np.moveaxis(df_X,0,-1)\n",
        "df_Y = np.moveaxis(df_Y,0,-1)\n",
        "print(\"\\n \\n X shape :\", df_X.shape)\n",
        "print(\" Y shape :\", df_Y.shape)\n",
        "\n",
        "\n",
        "# Split the dataset\n",
        "N_samples = df_X.shape[0]\n",
        "train_frac = 0.6\n",
        "val_frac = 0.2\n",
        "test_frac = 0.2\n",
        "\n",
        "train_size = int(N_samples * train_frac)\n",
        "val_size = int(N_samples * val_frac)\n",
        "test_size = N_samples - train_size - val_size\n",
        "print(\"\\nData train rows      :\", train_size)\n",
        "print(\"Data validation rows :\", val_size)\n",
        "print(\"Data test rows       :\", test_size)\n",
        "\n",
        "train_X, val_X, test_X, train_y, val_y, test_y = split_data(df_X, df_Y, train_frac, val_frac, test_frac, N_samples)\n",
        "print(\"\\nData shape\")\n",
        "print(\"train_X :\", train_X.shape)\n",
        "print(\"val_X   :\", val_X.shape)\n",
        "print(\"test_X  :\", test_X.shape)\n",
        "print(\"train_y :\", train_y.shape)\n",
        "print(\"val_y   :\", val_y.shape)\n",
        "print(\"test_y  :\", test_y.shape)"
      ],
      "metadata": {
        "id": "xL7ASLhYkLl_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xL7ASLhYkLl_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter tunning"
      ],
      "metadata": {
        "id": "wcANl1wYcEp3"
      },
      "id": "wcANl1wYcEp3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tanh"
      ],
      "metadata": {
        "id": "HAoB1xYKrxZ1"
      },
      "id": "HAoB1xYKrxZ1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will conduct several experiments for the activation function on the Conv2D Block. In this case, for the first layer we will use Tanh. However, for the second layer we will try using Leaky ReLU, ELU and Tanh."
      ],
      "metadata": {
        "id": "26hRmxMF3P86"
      },
      "id": "26hRmxMF3P86"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. tanh_lrelu"
      ],
      "metadata": {
        "id": "MI51sUaiQFOb"
      },
      "id": "MI51sUaiQFOb"
    },
    {
      "cell_type": "code",
      "source": [
        "### Hyperparameter Tuning Combinations\n",
        "np.random.seed(100)\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {'batch_size' : [16, 32, 64],\n",
        "              'learning_rate' : [0.0001, 0.0003, 0.0005, 0.0007, 0.0009],\n",
        "              'hid_size' : [4, 8]}\n",
        "\n",
        "# create parameter combinations\n",
        "param_combinations = list(ParameterGrid(param_grid))\n",
        "params_detail = pd.DataFrame(param_combinations)\n",
        "params_detail"
      ],
      "metadata": {
        "id": "v7OM4NnhQFOn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "v7OM4NnhQFOn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Model\n",
        "\n",
        "models = []\n",
        "history_models = []\n",
        "\n",
        "for params in param_combinations:\n",
        "    print(params)\n",
        "\n",
        "    results = train_conv2d_scinet(\n",
        "                        X_train = train_X,\n",
        "                        y_train = train_y,\n",
        "                        X_val = val_X,\n",
        "                        y_val = val_y,\n",
        "                        X_test = test_X,\n",
        "                        y_test = test_y,\n",
        "                        epochs = 20,\n",
        "                        batch_size = params['batch_size'],\n",
        "                        X_LEN = 24, # window size\n",
        "                        Y_LEN = [24], # horizon\n",
        "                        output_dim = [5], # locations\n",
        "                        selected_columns = None,\n",
        "                        hid_size = params['hid_size'],\n",
        "                        num_levels = 2,\n",
        "                        kernel = 5,\n",
        "                        dropout = 0.5,\n",
        "                        loss_weights = [1],\n",
        "                        learning_rate = params['learning_rate'],\n",
        "                        filters = [16, 8],\n",
        "                        kernel_size = (7, 4), # 7 days, 4 variables\n",
        "                        activation_func = ['tanh','leaky_relu'])\n",
        "\n",
        "    models.append(results[0])\n",
        "    history_models.append(results[1])"
      ],
      "metadata": {
        "id": "xS_L0ZIsQFOp"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xS_L0ZIsQFOp"
    },
    {
      "cell_type": "code",
      "source": [
        "# the list of model names\n",
        "model_name = []\n",
        "for i in range(len(models)):\n",
        "  model_name.append('Model {}'.format(i))\n",
        "model_name"
      ],
      "metadata": {
        "id": "VqrFOQ92CdBF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "VqrFOQ92CdBF"
    },
    {
      "cell_type": "code",
      "source": [
        "# plot history\n",
        "for i in range(len(models)):\n",
        "  learning_curve(history_models[i], model_name[i])"
      ],
      "metadata": {
        "id": "ybMQzn1kQFOr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ybMQzn1kQFOr"
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "for idx, model in enumerate(models):\n",
        "  model.save_weights('/content/drive/MyDrive/Bobot/tanh_lrelu_{}.h5'.format(idx))"
      ],
      "metadata": {
        "id": "ImkjBj9XCt8c"
      },
      "id": "ImkjBj9XCt8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict validation data"
      ],
      "metadata": {
        "id": "lcb-VbNUNuq7"
      },
      "id": "lcb-VbNUNuq7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model to load the weights\n",
        "model_ = []\n",
        "for params in param_combinations:\n",
        "    print(params)\n",
        "    model =  Conv2D_SCINet( output_len = [24],\n",
        "                       output_dim = [5],\n",
        "                       input_len = 24,\n",
        "                       input_dim = train_X.shape[2] * train_X.shape[3],\n",
        "                       x_features = train_X.shape[2],\n",
        "                       locations = train_X.shape[3],\n",
        "                       selected_columns = None,\n",
        "                       hid_size = params['hid_size'],\n",
        "                       num_levels = 2,\n",
        "                       kernel = 5,\n",
        "                       dropout = 0.5,\n",
        "                       loss_weights = [1],\n",
        "                       learning_rate = params['learning_rate'],\n",
        "                       probabilistic = False,\n",
        "                       filters = [16, 8],\n",
        "                       kernel_size = (7, 4),\n",
        "                       activation_func = ['tanh','leaky_relu'])\n",
        "    model = model.build_model()\n",
        "    model_.append(model)"
      ],
      "metadata": {
        "id": "DqjlLJ5mETQG"
      },
      "id": "DqjlLJ5mETQG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict data validation\n",
        "yhats_val = []\n",
        "for idx, model in enumerate(model_):\n",
        "    model.load_weights('/content/drive/MyDrive/Bobot/tanh_lrelu_{}.h5'.format(idx))\n",
        "    yhat = model.predict(val_X)\n",
        "    yhats_val.append(yhat)"
      ],
      "metadata": {
        "id": "TvYKCoSAQFOr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "TvYKCoSAQFOr"
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "inv_yhat_val, inv_val_y = invers_trans(model_name, yhats_val, val_y)\n",
        "eval_metrics_val = evaluation_metrics(inv_val_y, inv_yhat_val, model_name)\n",
        "\n",
        "# the best model\n",
        "print('The best model')\n",
        "the_best_model(eval_metrics_val)"
      ],
      "metadata": {
        "id": "Mr7pytjLQFOt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Mr7pytjLQFOt"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metrics_val"
      ],
      "metadata": {
        "id": "YOJggfEyDfCS"
      },
      "id": "YOJggfEyDfCS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict test data"
      ],
      "metadata": {
        "id": "DahnyePCOCUw"
      },
      "id": "DahnyePCOCUw"
    },
    {
      "cell_type": "code",
      "source": [
        "# predict data test\n",
        "yhats = []\n",
        "for idx, model in enumerate(model_):\n",
        "    model.load_weights('/content/drive/MyDrive/Bobot/tanh_lrelu_{}.h5'.format(idx))\n",
        "    yhat = model.predict(test_X)\n",
        "    yhats.append(yhat)"
      ],
      "metadata": {
        "id": "B9O4eXhmDNUC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "B9O4eXhmDNUC"
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "inv_yhat, inv_test_y = invers_trans(model_name, yhats, test_y)\n",
        "eval_metrics = evaluation_metrics(inv_test_y, inv_yhat, model_name)\n",
        "\n",
        "# the best model\n",
        "print('The best model')\n",
        "the_best_model(eval_metrics)"
      ],
      "metadata": {
        "id": "IdnI5b8VDSv8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "IdnI5b8VDSv8"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metrics"
      ],
      "metadata": {
        "id": "U7AfwbLdQFOu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "U7AfwbLdQFOu"
    },
    {
      "cell_type": "code",
      "source": [
        "params_detail"
      ],
      "metadata": {
        "id": "WuR2ir41QFOu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "WuR2ir41QFOu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. tanh_elu"
      ],
      "metadata": {
        "id": "msxvRHeNeUq3"
      },
      "id": "msxvRHeNeUq3"
    },
    {
      "cell_type": "code",
      "source": [
        "### Hyperparameter Tuning Combinations\n",
        "np.random.seed(100)\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {'batch_size' : [16, 32, 64],\n",
        "              'learning_rate' : [0.0001, 0.0003, 0.0005, 0.0007, 0.0009],\n",
        "              'hid_size' : [4, 8]}\n",
        "\n",
        "# create parameter combinations\n",
        "param_combinations = list(ParameterGrid(param_grid))\n",
        "params_detail = pd.DataFrame(param_combinations)\n",
        "params_detail"
      ],
      "metadata": {
        "id": "su_-9l1OeUrf"
      },
      "execution_count": null,
      "outputs": [],
      "id": "su_-9l1OeUrf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Model\n",
        "\n",
        "models = []\n",
        "history_models = []\n",
        "\n",
        "for params in param_combinations:\n",
        "    print(params)\n",
        "\n",
        "    results = train_conv2d_scinet(\n",
        "                        X_train = train_X,\n",
        "                        y_train = train_y,\n",
        "                        X_val = val_X,\n",
        "                        y_val = val_y,\n",
        "                        X_test = test_X,\n",
        "                        y_test = test_y,\n",
        "                        epochs = 20,\n",
        "                        batch_size = params['batch_size'],\n",
        "                        X_LEN = 24, # window size\n",
        "                        Y_LEN = [24], # horizon\n",
        "                        output_dim = [5], # locations\n",
        "                        selected_columns = None,\n",
        "                        hid_size = params['hid_size'],\n",
        "                        num_levels = 2,\n",
        "                        kernel = 5,\n",
        "                        dropout = 0.5,\n",
        "                        loss_weights = [1],\n",
        "                        learning_rate = params['learning_rate'],\n",
        "                        filters = [16, 8],\n",
        "                        kernel_size = (7, 4), # 7 days, 4 variables\n",
        "                        activation_func = ['tanh','elu'])\n",
        "\n",
        "    models.append(results[0])\n",
        "    history_models.append(results[1])"
      ],
      "metadata": {
        "id": "YNGUF6AAeUrj"
      },
      "execution_count": null,
      "outputs": [],
      "id": "YNGUF6AAeUrj"
    },
    {
      "cell_type": "code",
      "source": [
        "# the list of model names\n",
        "model_name = []\n",
        "for i in range(len(models)):\n",
        "  model_name.append('Model {}'.format(i))\n",
        "model_name"
      ],
      "metadata": {
        "id": "lT86Zeg2eUrq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lT86Zeg2eUrq"
    },
    {
      "cell_type": "code",
      "source": [
        "# plot history\n",
        "for i in range(len(models)):\n",
        "  learning_curve(history_models[i], model_name[i])"
      ],
      "metadata": {
        "id": "fRAbmkGleUrt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fRAbmkGleUrt"
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "for idx, model in enumerate(models):\n",
        "  model.save_weights('/content/drive/MyDrive/Bobot/tanh_elu_{}.h5'.format(idx))"
      ],
      "metadata": {
        "id": "SCDmeBqlLitf"
      },
      "id": "SCDmeBqlLitf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict validation data"
      ],
      "metadata": {
        "id": "7ndE-Dw8OG7E"
      },
      "id": "7ndE-Dw8OG7E"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model to load the weights\n",
        "model_ = []\n",
        "for params in param_combinations:\n",
        "    print(params)\n",
        "    model =  Conv2D_SCINet( output_len = [24],\n",
        "                       output_dim = [5],\n",
        "                       input_len = 24,\n",
        "                       input_dim = train_X.shape[2] * train_X.shape[3],\n",
        "                       x_features = train_X.shape[2],\n",
        "                       locations = train_X.shape[3],\n",
        "                       selected_columns = None,\n",
        "                       hid_size = params['hid_size'],\n",
        "                       num_levels = 2,\n",
        "                       kernel = 5,\n",
        "                       dropout = 0.5,\n",
        "                       loss_weights = [1],\n",
        "                       learning_rate = params['learning_rate'],\n",
        "                       probabilistic = False,\n",
        "                       filters = [16, 8],\n",
        "                       kernel_size = (7, 4),\n",
        "                       activation_func = ['tanh','elu'])\n",
        "    model = model.build_model()\n",
        "    model_.append(model)"
      ],
      "metadata": {
        "id": "LGWy-VVKNcK8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "LGWy-VVKNcK8"
    },
    {
      "cell_type": "code",
      "source": [
        "# predict data validation\n",
        "yhats_val = []\n",
        "for idx, model in enumerate(model_):\n",
        "    model.load_weights('/content/drive/MyDrive/Bobot/tanh_elu_{}.h5'.format(idx))\n",
        "    yhat = model.predict(val_X)\n",
        "    yhats_val.append(yhat)"
      ],
      "metadata": {
        "id": "aeNNisT2NcLI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aeNNisT2NcLI"
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "inv_yhat_val, inv_val_y = invers_trans(model_name, yhats_val, val_y)\n",
        "eval_metrics_val = evaluation_metrics(inv_val_y, inv_yhat_val, model_name)\n",
        "\n",
        "# the best model\n",
        "print('The best model')\n",
        "the_best_model(eval_metrics_val)"
      ],
      "metadata": {
        "id": "_l7oQsDuNcLI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_l7oQsDuNcLI"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metrics_val"
      ],
      "metadata": {
        "id": "6bHXjzZENcLI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6bHXjzZENcLI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict test data"
      ],
      "metadata": {
        "id": "Ll8ZPs3TOKRH"
      },
      "id": "Ll8ZPs3TOKRH"
    },
    {
      "cell_type": "code",
      "source": [
        "# predict data test\n",
        "yhats = []\n",
        "for idx, model in enumerate(models):\n",
        "    model.load_weights('/content/drive/MyDrive/Bobot/tanh_elu_{}.h5'.format(idx))\n",
        "    yhat = model.predict(test_X)\n",
        "    yhats.append(yhat)"
      ],
      "metadata": {
        "id": "79Tya41meUr2"
      },
      "execution_count": null,
      "outputs": [],
      "id": "79Tya41meUr2"
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "inv_yhat, inv_test_y = invers_trans(model_name, yhats, test_y)\n",
        "eval_metrics = evaluation_metrics(inv_test_y, inv_yhat, model_name)"
      ],
      "metadata": {
        "id": "ktb-N3ZQeUr3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ktb-N3ZQeUr3"
    },
    {
      "cell_type": "code",
      "source": [
        "# the best model\n",
        "print('The best model')\n",
        "the_best_model(eval_metrics)"
      ],
      "metadata": {
        "id": "PHWEt42NeUr4"
      },
      "execution_count": null,
      "outputs": [],
      "id": "PHWEt42NeUr4"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metrics"
      ],
      "metadata": {
        "id": "pczo8_S0eUr-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pczo8_S0eUr-"
    },
    {
      "cell_type": "code",
      "source": [
        "params_detail"
      ],
      "metadata": {
        "id": "gX_taHUleUr_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "gX_taHUleUr_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. tanh_tanh"
      ],
      "metadata": {
        "id": "FZJN1FL_luyP"
      },
      "id": "FZJN1FL_luyP"
    },
    {
      "cell_type": "code",
      "source": [
        "### Hyperparameter Tuning Combinations\n",
        "np.random.seed(100)\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {'batch_size' : [16, 32, 64],\n",
        "              'learning_rate' : [0.0001, 0.0003, 0.0005, 0.0007, 0.0009],\n",
        "              'hid_size' : [4, 8]}\n",
        "\n",
        "# create parameter combinations\n",
        "param_combinations = list(ParameterGrid(param_grid))\n",
        "params_detail = pd.DataFrame(param_combinations)\n",
        "params_detail"
      ],
      "metadata": {
        "id": "F9ridF7SluyV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "F9ridF7SluyV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Model\n",
        "\n",
        "models = []\n",
        "history_models = []\n",
        "\n",
        "for params in param_combinations:\n",
        "    print(params)\n",
        "\n",
        "    results = train_conv2d_scinet(\n",
        "                        X_train = train_X,\n",
        "                        y_train = train_y,\n",
        "                        X_val = val_X,\n",
        "                        y_val = val_y,\n",
        "                        X_test = test_X,\n",
        "                        y_test = test_y,\n",
        "                        epochs = 20,\n",
        "                        batch_size = params['batch_size'],\n",
        "                        X_LEN = 24, # window size\n",
        "                        Y_LEN = [24], # horizon\n",
        "                        output_dim = [5], # locations\n",
        "                        selected_columns = None,\n",
        "                        hid_size = params['hid_size'],\n",
        "                        num_levels = 2,\n",
        "                        kernel = 5,\n",
        "                        dropout = 0.5,\n",
        "                        loss_weights = [1],\n",
        "                        learning_rate = params['learning_rate'],\n",
        "                        filters = [16, 8],\n",
        "                        kernel_size = (7, 4), # 7 days, 4 variables\n",
        "                        activation_func = ['tanh','tanh'])\n",
        "\n",
        "    models.append(results[0])\n",
        "    history_models.append(results[1])"
      ],
      "metadata": {
        "id": "vTqO56ziluyW"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vTqO56ziluyW"
    },
    {
      "cell_type": "code",
      "source": [
        "# the list of model names\n",
        "model_name = []\n",
        "for i in range(len(models)):\n",
        "  model_name.append('Model {}'.format(i))\n",
        "model_name"
      ],
      "metadata": {
        "id": "6zmGx8IpluyX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6zmGx8IpluyX"
    },
    {
      "cell_type": "code",
      "source": [
        "# plot history\n",
        "for i in range(len(models)):\n",
        "  learning_curve(history_models[i], model_name[i])"
      ],
      "metadata": {
        "id": "tl3HBdH_luyX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "tl3HBdH_luyX"
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "for idx, model in enumerate(models):\n",
        "  model.save_weights('/content/drive/MyDrive/Bobot/tanh_tanh_{}.h5'.format(idx))"
      ],
      "metadata": {
        "id": "xXNy0Z0OLzcq"
      },
      "id": "xXNy0Z0OLzcq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict validation data"
      ],
      "metadata": {
        "id": "myOewEv4PbEt"
      },
      "id": "myOewEv4PbEt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model to load the weights\n",
        "model_ = []\n",
        "for params in param_combinations:\n",
        "    print(params)\n",
        "    model =  Conv2D_SCINet( output_len = [24],\n",
        "                       output_dim = [5],\n",
        "                       input_len = 24,\n",
        "                       input_dim = train_X.shape[2] * train_X.shape[3],\n",
        "                       x_features = train_X.shape[2],\n",
        "                       locations = train_X.shape[3],\n",
        "                       selected_columns = None,\n",
        "                       hid_size = params['hid_size'],\n",
        "                       num_levels = 2,\n",
        "                       kernel = 5,\n",
        "                       dropout = 0.5,\n",
        "                       loss_weights = [1],\n",
        "                       learning_rate = params['learning_rate'],\n",
        "                       probabilistic = False,\n",
        "                       filters = [16, 8],\n",
        "                       kernel_size = (7, 4),\n",
        "                       activation_func = ['tanh','tanh'])\n",
        "    model = model.build_model()\n",
        "    model_.append(model)"
      ],
      "metadata": {
        "id": "WJbNIsSoNed8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "WJbNIsSoNed8"
    },
    {
      "cell_type": "code",
      "source": [
        "# predict data validation\n",
        "yhats_val = []\n",
        "for idx, model in enumerate(model_):\n",
        "    model.load_weights('/content/drive/MyDrive/Bobot/tanh_tanh_{}.h5'.format(idx))\n",
        "    yhat = model.predict(val_X)\n",
        "    yhats_val.append(yhat)"
      ],
      "metadata": {
        "id": "fem3BvYWNeeG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fem3BvYWNeeG"
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "inv_yhat_val, inv_val_y = invers_trans(model_name, yhats_val, val_y)\n",
        "eval_metrics_val = evaluation_metrics(inv_val_y, inv_yhat_val, model_name)\n",
        "\n",
        "# the best model\n",
        "print('The best model')\n",
        "the_best_model(eval_metrics_val)"
      ],
      "metadata": {
        "id": "ORxCF0WENeeG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ORxCF0WENeeG"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metrics_val"
      ],
      "metadata": {
        "id": "I4eAr6cNNeeH"
      },
      "execution_count": null,
      "outputs": [],
      "id": "I4eAr6cNNeeH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict test data"
      ],
      "metadata": {
        "id": "qRCi105ePlj6"
      },
      "id": "qRCi105ePlj6"
    },
    {
      "cell_type": "code",
      "source": [
        "# predict data test\n",
        "yhats = []\n",
        "for idx, model in enumerate(model_):\n",
        "    model.load_weights('/content/drive/MyDrive/Bobot/tanh_tanh_{}.h5'.format(idx))\n",
        "    yhat = model.predict(test_X)\n",
        "    yhats.append(yhat)"
      ],
      "metadata": {
        "id": "zpbGQWjsluyZ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "zpbGQWjsluyZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "inv_yhat, inv_test_y = invers_trans(model_name, yhats, test_y)\n",
        "eval_metrics = evaluation_metrics(inv_test_y, inv_yhat, model_name)"
      ],
      "metadata": {
        "id": "XsU5g86Bluya"
      },
      "execution_count": null,
      "outputs": [],
      "id": "XsU5g86Bluya"
    },
    {
      "cell_type": "code",
      "source": [
        "# the best model\n",
        "print('The best model')\n",
        "the_best_model(eval_metrics)"
      ],
      "metadata": {
        "id": "z5owUVYoluyb"
      },
      "execution_count": null,
      "outputs": [],
      "id": "z5owUVYoluyb"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metrics"
      ],
      "metadata": {
        "id": "Qt-uPzP_luyc"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Qt-uPzP_luyc"
    },
    {
      "cell_type": "code",
      "source": [
        "params_detail"
      ],
      "metadata": {
        "id": "i-9eSDO7luyd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "i-9eSDO7luyd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#End of notebook"
      ],
      "metadata": {
        "id": "EFFGW3gY12Qg"
      },
      "id": "EFFGW3gY12Qg"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b80aa8293e98206ec1521bb25d120a454bd9470ae610e9b13876565475d9d2ee"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
